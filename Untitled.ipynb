{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04de82af",
   "metadata": {},
   "source": [
    "## N-gram Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aeabba1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\qfu88\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "nltk_data_path = \"Data/punkt.zip\"\n",
    "if nltk_data_path not in nltk.data.path:\n",
    "    nltk.data.path.append(nltk_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9851a1",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc75bbc",
   "metadata": {},
   "source": [
    "Create a function to load the data, This function should accomplish the following:\n",
    "\n",
    "Extract sentences from the data file. Of course, depending on the nature of the task at hand, what constitutes a sentence can vary. In the context of this assignment, we will define a sentence as just a line of a sonnet, regardless of the punctuation at end. In addition, we will ignore the boundaries of the sonnets --- that is, we are not dealing with  154\n",
    "  individual sonnets but rather  154√ó14=2156\n",
    "  sentences (actually only  2155\n",
    "  sentences, as Sonnet 99 has 15 lines but Sonnet 126 has only 12). We encourage you to explore alternative definitions of a sentence on your own; for example, an entire sonnet could be modelled as a sentence. Finally, make sure that the newline character \\n at the end of each line is dropped.\n",
    "  \n",
    "  \n",
    "Tokenise each extracted sentence. While it's ambiguous what a sentence is, what constitutes a \"word\" is even more task-dependent. Do punctuations count as \"words\"? Are two \"words\" with the same spelling but different casing considered identical? Since what a text file contains is nothing more than a squence of characters, there are many possible ways of grouping these characters to form \"words\" that are subsequently taken as input by a program. To distinguish what's actually taken as input by a program from a linguistic word, we call the former a token. The process of producing a list of tokens out of a sentence is then called tokenisation. At this step, you will first lower-case each sentence extracted from the previous step entirely and then tokenise each lower-cased sentence. You may use any tokeniser of your choice, such as word_tokenize from the nltk library. The grading of the assignment doesn't depend on your choice of the tokeniser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8214fc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load text data from a file and produce a list of token lists\n",
    "    \"\"\"\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    with open(\"Data\\THE_SONNETS.txt\", \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        \n",
    "        for line in lines:\n",
    "            # Remove the newline character at the end\n",
    "            line = line.strip()\n",
    "            # Skip empty lines or lines that are just numbers (indicating sonnet numbers)\n",
    "            if not line or line.isdigit():\n",
    "                continue\n",
    "            \n",
    "            line = line.lower()\n",
    "            # Tokenize the sentence\n",
    "            tokens = word_tokenize(line)\n",
    "            \n",
    "            sentences.append(tokens)\n",
    "\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb84e5c6",
   "metadata": {},
   "source": [
    "### Build vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57c91eb",
   "metadata": {},
   "source": [
    "Next, we need a \"vocabulary\" that contains all the unique tokens. Moreover, as mentioned in the lecture, we often pad a sentence with <s> and </s> to indicate its start and end when working with n-gram language models; therefore, these two special tokens should also be included in our vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "240381ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences):\n",
    "    \"\"\"\n",
    "    Take a list of sentences and return a vocab\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab = set()\n",
    "    for sentence in sentences:\n",
    "        for token in sentence:\n",
    "            vocab.add(token)\n",
    "    vocab.add(\"<s>\")\n",
    "    vocab.add(\"</s>\")\n",
    "    \n",
    "    vocab = list(vocab)\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2ea714",
   "metadata": {},
   "source": [
    "###  Generate all  ùëõ-grams\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848138d4",
   "metadata": {},
   "source": [
    "Now let's write a function to generate all  ùëõ\n",
    " -grams for each sentence. This can be accomplished in two steps:\n",
    "\n",
    "Pad each sentence with <s> and </s> for  ùëõ‚â•2\n",
    " . You need  ùëõ‚àí1\n",
    "  paddings on both ends of a sentence, so that there are two  ùëõ\n",
    " -grams that model the first and the last token, respectively. You may implement the padding function yourself or use the pad_both_ends function from the nltk library.\n",
    "Generate  ùëõ\n",
    " -grams on the padded sentences. Check out the ngrams function from nltk. For a sentence with  ‚Ñì\n",
    "  tokens excluding paddings, the maximum number of possible  ùëõ\n",
    " -grams generated from its padded version should be  ‚Ñì+ùëõ‚àí1\n",
    " . Think about why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa306798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "\n",
    "def pad_sentence(sentence, n):\n",
    "    \n",
    "    return ['<s>'] * (n-1) + sentence + ['</s>'] * (n-1)\n",
    "\n",
    "def build_ngrams(n, sentences):\n",
    "    \"\"\"\n",
    "    Take a list of unpadded sentences and create all n-grams as specified by the argument \"n\" for each sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    all_ngrams = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        padded_sentence = list(pad_sentence(sentence, n))\n",
    "        \n",
    "        all_ngrams.append(list(ngrams(padded_sentence, n)))\n",
    "       \n",
    "    \n",
    "    return all_ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633d682d",
   "metadata": {},
   "source": [
    "### Guess the next token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9ae02c",
   "metadata": {},
   "source": [
    "Assume we are now working with bi-grams. What is the most likely token that comes after the sequence \"s s s\", and how likely? Remember that a bi-gram language model is essentially a first-order Markov Chain. So, what determines the next state in a first-order Markov Chain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11e70004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "def bigram_next_token(start_tokens=(\"<s>\", ) * 3):\n",
    "    \"\"\"\n",
    "    Take some starting tokens and produce the most likely token that follows under a bi-gram model\n",
    "    \"\"\"\n",
    "    \n",
    "    next_token, prob = None, None\n",
    "    #load_data() in order to have sentences to work with \n",
    "    #create ngrams using build_ngrams() function \n",
    "    #create empty list for tokens \n",
    "\n",
    "    #iterate through the ngrams created from the build_ngrams() function \n",
    "        #iterate through the bigrams in ngrams \n",
    "            #if bigrams index 0 is equal to last start_tokens\n",
    "                #append the last bigram to empty list created for tokens \n",
    "\n",
    "    #HINT: use .most_common from Counter of tokens to identify what the next token would be \n",
    "    #calculate the probability of the token \n",
    "  \n",
    "    #load data and create bi-grams\n",
    "    sentences = load_data()\n",
    "    bi_grams = build_ngrams(2, sentences)\n",
    "    \n",
    "    \n",
    "    #Flatten the list of lists to have a flat list of bi-grams\n",
    "    flat_bigrams = [bigram for sublist in bi_grams for bigram in sublist]\n",
    "    \n",
    "    #Create a Counter to count occurrences of each bi-gram\n",
    "    bigram_counter = Counter(flat_bigrams)\n",
    "    \n",
    "    # Extract the condition token, i.e., the token we want to compute the conditional probability on\n",
    "    condition_token = start_tokens[-1]\n",
    "    \n",
    "    # Create a dictionary to store conditional probabilities\n",
    "    cond_prob = {}\n",
    "    for bigram, count in bigram_counter.items():\n",
    "        if bigram[0] == condition_token:\n",
    "            cond_prob[bigram[1]] = count / sum([val for key, val in bigram_counter.items() if key[0] == condition_token])\n",
    "    \n",
    "    # Step 5: Identify the most likely next token and its probability\n",
    "    next_token, prob = max(cond_prob.items(), key=lambda x: x[1])\n",
    "    \n",
    "\n",
    "    \n",
    "    #raise NotImplementedError()\n",
    "    \n",
    "    return next_token, prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf6d4f3",
   "metadata": {},
   "source": [
    "### Train an  ùëõ-gram language model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17ddd94",
   "metadata": {},
   "source": [
    "Now we are well positioned to start training an  ùëõ\n",
    " -gram language model. We can fit a language model using the MLE class from nltk.lm. It requires two inputs: a list of all  ùëõ\n",
    " -grams for each sentence and a vocabulary, both of which you have already written a function to build. Now it's time to put them together to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad34a6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import MLE\n",
    "\n",
    "def train_ngram_lm(n):\n",
    "    \"\"\"\n",
    "    Train a n-gram language model as specified by the argument \"n\"\n",
    "    \"\"\"\n",
    "    \n",
    "    lm = MLE(n)\n",
    "    \n",
    "    # Load the sentences\n",
    "    sentences = load_data()\n",
    "    \n",
    "    # Build n-grams, get a flat list of n-grams\n",
    "    all_ngrams = [gram for sent in build_ngrams(n, sentences) for gram in sent]\n",
    "    \n",
    "    # Build vocabulary\n",
    "    vocab = build_vocab(sentences)\n",
    "    \n",
    "    # Fit the model with n-grams\n",
    "    lm.fit([all_ngrams], vocabulary_text=vocab)\n",
    "    \n",
    "    return lm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c3cff2",
   "metadata": {},
   "source": [
    "FINALLY, are you ready to compose sonnets like the real Shakespeare?! We provide some starter code below, but absolutely feel free to modify any parts of it on your own. It'd be interesting to see how the \"authenticity\" of the sonnets is related to the parameter  ùëõ\n",
    " . Do the sonnets feel more Shakespeare when you increase  ùëõ\n",
    " ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c907bb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be as thy sweet self prove .\n",
      "even so doth she abuse me ,\n",
      "within thine own deep sunken eyes , her\n",
      "on your broad main doth wilfully appear .\n",
      "hath motion , and mine only care ,\n",
      "straight in her heart did mercy come ,\n",
      "and often is his due ,\n",
      "whether we are mended , or more\n",
      "he lends thee virtue , and therefore have\n",
      "no longer mourn for me than spurring to\n",
      "why with the time do i find ,\n",
      "when most i wink then do thy office\n",
      "and him as fist doth bind .\n",
      "strikes each in each by mutual ordering ;\n"
     ]
    }
   ],
   "source": [
    "# Every time it runs, depending on how drunk it is, a different sonnet is written. \n",
    "n = 3\n",
    "num_lines = 14\n",
    "num_words_per_line = 8\n",
    "text_seed = [\"<s>\"] * (n - 1)\n",
    "\n",
    "lm = train_ngram_lm(n)\n",
    "\n",
    "sonnet = []\n",
    "while len(sonnet) < num_lines:\n",
    "    while True:  # keep generating a line until success\n",
    "        try:\n",
    "            line = lm.generate(num_words_per_line, text_seed=text_seed)\n",
    "        except ValueError:  # the generation is not always successful. need to capture exceptions\n",
    "            continue\n",
    "        else:\n",
    "            line = [x for x in line if x not in [\"<s>\", \"</s>\"]]\n",
    "            sonnet.append(\" \".join(line))\n",
    "            break\n",
    "\n",
    "# pretty-print your sonnet\n",
    "print(\"\\n\".join(sonnet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5791a02d",
   "metadata": {},
   "source": [
    "### Hidden Markov Models (HMMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13787ede",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
